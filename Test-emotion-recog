''' import matplotlib
import cv2
import numpy as np
from keras.models import model_from_json

emot_dict = {0: "Anger", 1: "Disgust", 2: "Fear", 3: "Happy", 4: "Neutral", 5: "Sad", 6: "Surprise"}

with open('model/emotion_recognition.json', 'r') as json_file:
    loaded_model_json = json_file.read()

emotion_model = model_from_json(loaded_model_json)

emotion_model.load_weights("model/emotion_recognition.h5")

cap= cv2.VideoCapture(0) #to check live from camera
#otherwise pass the path of the vid

while (cap.isOpened()):
    ret, frame = cap.read()
    frame = cv2.resize(frame,(1280, 720))
    if not ret:
        break
    face_detect = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')
    gray_fr = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    face_numb = face_detect.detectMultiScale(gray_fr, scaleFactor=1.3, minNeighbors=5)

    for(x,y,w,h) in face_numb:
        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0,255,0), 4)
        roi_gf = gray_fr[y: y+h, x: x+w]
        cropped_im = np.expand_dims(np.expand_dims(cv2.resize(roi_gf, (48,48)), -1), 0)

        predict = emotion_model.predict(cropped_im)
        maxindex = int(np.argmax(predict))
        cv2.putText(frame, emot_dict[maxindex], (x+5, y-20), cv2.FONT_ITALIC, 1, (255,0,0), 2, cv2.LINE_AA)

    #if len(face_numb) > 0:
     #   predict = emotion_model.predict(cropped_im)
      #  maxindex = int(np.argmax(predict))
       # print(emot_dict[maxindex])

    #if cv2.waitKey(1) & 0xFF == ord('q'):
     #   break
    cv2.imshow('Emotion Detection', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()'''

import cv2
import numpy as np
from keras.models import model_from_json

emot_dict = {
    0: "Anger",
    1: "Disgust",
    2: "Fear",
    3: "Happy",
    4: "Neutral",
    5: "Sad",
    6: "Surprise"
}

# Load the emotion recognition model
with open('model/emotion_recognition.json', 'r') as json_file:
    loaded_model_json = json_file.read()

emotion_model = model_from_json(loaded_model_json)
emotion_model.load_weights("model/emotion_recognition.h5")

# Initialize video capture
cap = cv2.VideoCapture(0)  # Use the camera, update the index if needed

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    
    # Convert frame to grayscale for face detection
    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    
    # Detect faces
    face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')
    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)
    
    for (x, y, w, h) in faces:
        # Extract face region and preprocess for the model
        face_roi = gray_frame[y: y + h, x: x + w]
        cropped_face = np.expand_dims(np.expand_dims(cv2.resize(face_roi, (48, 48)), -1), 0)
        
        # Make emotion prediction
        prediction = emotion_model.predict(cropped_face)
        predicted_emotion = emot_dict[np.argmax(prediction)]
        
        # Draw bounding box and label on the frame
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.putText(frame, predicted_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
    
    cv2.imshow('Emotion Detection', frame)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
